# Lab 7.2 - Revision Questions

# a) Polynomial Regression with TensorFlow

# Question #1:
# - What is the activation function of this neuron? Why?
# Answer #1:
# - The Activation Function used in this Neuron was the Linear Activation Function
#   because this is a Linear Regression problem, where it is intended to
#   predict a Numerical Continuous Value, which can be a negative one;

# Question #2:
# - What is the shape of the weights matrix for the neurons?
# - Does this matrix include the w_0 parameter in the equation above?
# Answer #2:
# - The shape of the weights matrix is (degree, 1), i.e.,
#   a column-vector with n elements, where n is the number of degrees used,
#   since this neuron with a Linear Activation Function, follows the form of
#   a Polynomial Equation, z = w_0 + (w_1 x x) + (w_2 x x^2) + (w_3 x x^3) + ...,
#   so it have a weight for each expanded feature, which will result on n final features,
#   excluding the weight w_0;
# - No, this matrix (or, column-vector) for the weights, does not include the weight w_0,
#   because it corresponds to the bias variable;

# Question #3:
# - What loss function did you use?
# - What is the justification for using this loss function?
# Answer #3:
# - For the Loss Function of this problem, it is used the Mean Squared Error (M.S.E.);
# - The Mean Squared Error (M.S.E.) is the more adequate for this problem, since it finds
#   the average squared difference between the predicted ys (Prediction_Labels)
#   and the true ys (True_Labels) for Numerical Continuous Values;

-------------------------------------------------------------------------------"

# b) Distinguish Odd and Even Digits in MNIST

# Question #1:
# - Which activation function did you use in the hidden layers?
# - Why?
# Answer #1:
# - In the Hidden Layers, it was used the Rectified Linear Unit (ReLU) Activation Function,
#   since it rectifies and solves the Vanishing Gradient Problem, in which, for Deep Neural Networks,
#   it cannot propagate useful gradient information from the output layer to the initial layers,
#   due to the small derivatives resulted from some other Activation Functions, such as,
#   the Sigmoid or Softmax Activation Functions;

# Question #2:
# - How many neurons and which activation function did you use in the output of your network?
# - Why?
# Answer #2:
# - It was used 6 Neurons, in total, 5 for the Hidden Layers and 1 for the Output Layer,
#   because the Convolutional Neural Networks (C.N.N.) are Deep Classifiers and
#   need more Non-Linear Transformations, to extract more representations of the initial features,
#   and since this is a problem with just two classes (Odd and Even),
#   it can be considered a Binary Classification problem, for which, the Sigmoid Activation Function
#   is more adequate;

# Question #3:
# - Which loss function did you use in this case?
# Answer #3:
# - Since this is a problem with just two classes (Odd and Even) and a Binary Classification problem,
#   where is used a final Sigmoid Classifier in the Output Layer, the Loss Function in this case,
#   is a Binary Cross-Entropy Loss Function;

-------------------------------------------------------------------------------"

# c) Generative Adversarial Network (GAN) and Generating Pok√©mon

# Question #1:
# - In a GAN, what is the input to the generative model?
# Answer #1:
# - The input of the Generative Model, in a Generative Adversarial Network (GAN),
#   is a vector of random values (i.e., noise), from which, will be generated
#   new instances (representations) of the original data;

# Question #2:
# - What is the role of the discriminative model?
# Answer #2:
# - The role of the Discriminative Model is to distinguish
#   the original (real) data, from the data created by
#   the Generative Model;
# - Basically, the Generative Model creates new data, from a random (noise) input,
#   and, the Discriminative Model distinguish the new data generated, from the original data.

# Question #3:
# - Why is the discriminative model frozen when training the generative model?
# Answer #3:
# - The Training of the Generative Adversarial Networks (GANs), consists in alternate between
#   the Training of the Generative Model and Training of the Discriminative Model,
#   from which, when is occurring the Training of the Generative Model,
#   the Discriminative Model is frozen, to allow it, to classify the generated images from
#   the random (noise) inputs, properly, as real images;

-------------------------------------------------------------------------------"